MODEL_NAME=Llama-3.1-8B-Instruct-q4f16_1-MLC

# For Llama3 70B model
# MODEL_NAME=Llama-3.1-70B-Instruct-q4f16_1-MLC

MODE=local
# Note: For using a different modes, please contact Rocket.Chat Sales team. https://www.rocket.chat/sales-contact
# The engine mode in MLC LLM. We provide three preset modes: local, interactive and server. The default mode is local.
# The choice of mode decides the values of “max_num_sequence”, “max_total_sequence_length” and “prefill_chunk_size” when they are not explicitly specified.

# 1. Mode “local” refers to the local server deployment which has low request concurrency. 
# So the max batch size will be set to 4, and max total sequence length and prefill chunk size are set to the context window size (or sliding window size) of the model.

# 2. Mode “interactive” refers to the interactive use of server, which has at most 1 concurrent request. 
# So the max batch size will be set to 1, and max total sequence length and prefill chunk size are set to the context window size (or sliding window size) of the model.

# 3. Mode “server” refers to the large server use case which may handle many concurrent request and want to use GPU memory as much as possible. 
# In this mode, we will automatically infer the largest possible max batch size and max total sequence length.

RUBRA_ORG=rocketchat

PLATFORM_TAG=cuda125
RELEASE=0.0.1
